{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "309a7707-125c-ae34-c2f7-8b058e5d85cb",
    "colab_type": "text",
    "id": "NREkPtdxwo78"
   },
   "source": [
    "### Linear regression from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this seminar you will implement a linear regression and train it using just the (stochastic) gradient descent, numpy and your brain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "aaade20d-8ecb-a881-7b92-caebb2387f44",
    "colab": {},
    "colab_type": "code",
    "id": "BuSfJY0Kwo79"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8954b4fe-71d6-9e6e-b212-2fae8108460e",
    "colab_type": "text",
    "id": "74GiCDnswo8D"
   },
   "source": [
    "**Acknowledgements**: I've primariliy used the material from [Andrew Ng's Coursera course][1] for this, but have also been helped by [this article][2] and [this one][3]. I used some code for the animation from [this kernel][4].\n",
    "\n",
    "  [1]: https://www.coursera.org/learn/machine-learning\n",
    "  [2]: http://tillbergmann.com/blog/python-gradient-descent.html\n",
    "  [3]: http://aimotion.blogspot.co.uk/2011/10/machine-learning-with-python-linear.html\n",
    "  [4]: https://www.kaggle.com/ronaldtroncoso20/d/START-UMD/gtd/global-terrorism-trends-animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the libraries and data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e04bc1ac-5df3-e5be-03b6-0d53e60cd2f8",
    "colab": {},
    "colab_type": "code",
    "id": "5bs1lqx5wo8E"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import http\n",
    "import io\n",
    "import requests\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/HSE-LaMBDA/MLatURL2019/master/day2/house-train.csv'\n",
    "try:\n",
    "    data = pd.read_csv(url)\n",
    "except http.client.IncompleteRead as e:\n",
    "    r = requests.get(url, timeout=10)\n",
    "    b = io.StringIO(r.text)\n",
    "    data = pd.read_csv(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate linear regression using an analytic solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will program the univariate least squares linear regression from scratch using an analytic formulae. \n",
    "\n",
    "Recall, that we're working with a problem of finding a weight $w$ such that\n",
    "$$\n",
    "\\sum_i (y_i - w \\cdot x_i)^2 \\to \\min,\n",
    "$$\n",
    "with $x_i, y_i \\in R^1$, i.e. they are numbers. \n",
    "\n",
    "Recall that our least squares solution is of the form: \n",
    "$$\n",
    "\\widehat{w}_1 = \n",
    "    \\frac{\\sum_{i=1}^{\\ell} (x_i - \\mu_x) (y_i - \\mu_y)}\n",
    "        {\\sum_{i=1}^{\\ell} (x_i - \\mu_x)^2}, \\\\\n",
    "\\widehat{w}_0 = \\mu_y - \\widehat{w}_1 \\mu_x\n",
    "$$\n",
    "with $\\mu_x = \\frac{1}{\\ell} \\sum_{i=1}^{\\ell} x_i,\n",
    "\\quad\n",
    "\\mu_y = \\frac{1}{\\ell} \\sum_{i=1}^{\\ell} y_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hints: use `np.mean` to compute the mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weights_univariate(X, y):\n",
    "    \"\"\"\n",
    "    Given feature array X [n_samples], target vector y [n_samples],\n",
    "    compute the optimal least squares solution using the formulae above.\n",
    "    \"\"\"\n",
    "    return <your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['GrLivArea']\n",
    "y = data['SalePrice']\n",
    "\n",
    "w = compute_weights_univariate(X, y)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How precise are we?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We commonly evaluate predictions using MSE errors. These are defined as:\n",
    "$$\n",
    "MSE(y, \\widehat{y}) = \\frac 1 \\ell \\sum_i (y_i - w^T x_i)^2\n",
    "$$\n",
    "as implemented either by `sklearn.metrics.mean_squared_error` or simply by `np.mean((y - y_pred) ** 2)`.\n",
    "\n",
    "We then use $\\sqrt{MSE(y, \\widehat{y})}$ to obtain numbers in the same dimensionality (units, not units squared)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_0, w_1 = w\n",
    "np.sqrt(\n",
    "    mean_squared_error(y, np.dot(X, w_1) + w_0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(nrows=1, ncols=2, figsize=(14, 4))\n",
    "axs[0].scatter(y, np.dot(X, w_1) + w_0, alpha=.3)\n",
    "axs[0].set_title('$y$ vs. $\\widehat{y} = X \\widehat{w}$')\n",
    "axs[1].plot(y - (np.dot(X, w_1) + w_0))\n",
    "axs[1].set_title('Error $y - X \\widehat{w}$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate linear regression using an analytic solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will program the **multi**variate least squares linear regression from scratch using an analytic formulae. \n",
    "\n",
    "Recall, that we're working with a problem of finding a weight $w$ such that\n",
    "$$\n",
    "||\\mathbf{y} - \\mathbf{X} \\mathbf{w}|| \\to \\min_{\\mathbf{w}},\n",
    "$$\n",
    "with $\\mathbf{x}_i \\in R^n$, i.e. features are no longer supposed to be single numbers, but vectors of numbers (but still $y_i \\in R^1$).\n",
    "\n",
    "Recall that our least squares solution is of the form: \n",
    "$$\n",
    "\\mathbf{w} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}.\n",
    "$$\n",
    "\n",
    "We could account for the non-zero mean case ($\\mathrm{E} \\mathbf{y} \\neq 0$) by either adding and subtracting the mean, or by using an additional feature in $\\mathbf{X}$ set to all ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: matrix inverse is computed by `np.linalg.inv`, and matrix-vector product by `np.dot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(X):\n",
    "    \"\"\"\n",
    "    Given feature array X [n_samples, n_features], preprocess it by\n",
    "    standardizing and appending a column of 1s.\n",
    "    \"\"\"\n",
    "    # First, standardize the input by subtracting mean and dividing by std\n",
    "    X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "    \n",
    "    # Append a free term (a column of 1s)\n",
    "    X = np.c_[np.ones(X.shape[0]), X]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weights_multivariate(X, y):\n",
    "    \"\"\"\n",
    "    Given feature array X [n_samples, 1], target vector y [n_samples],\n",
    "    compute the optimal least squares solution using the formulae above.\n",
    "    Don't forget the bias term!\n",
    "    \"\"\"\n",
    "    return <your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['GrLivArea']\n",
    "X = np.atleast_2d(X).T\n",
    "y = data['SalePrice']\n",
    "\n",
    "w = compute_weights_multivariate(X, y)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that we must run prepare_features(X) here, too, to obtain the exact same\n",
    "# feature representation of the input\n",
    "np.sqrt(\n",
    "    mean_squared_error(y, np.dot(prepare_features(X), w))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have implemented the weight computation procedure, you can test it using our dataset, using \n",
    "* different features (or even their subsets!),\n",
    "* different sub-samples of $x_i$s and $y_i$s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'LotFrontage',\n",
    "    'LotArea',\n",
    "    'MoSold',\n",
    "    'YrSold',\n",
    "    'OverallQual',\n",
    "    'OverallCond',\n",
    "    'YearBuilt',\n",
    "    'YearRemodAdd',\n",
    "    'MasVnrArea',\n",
    "    'BsmtFinSF1',\n",
    "    'GrLivArea'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in columns: \n",
    "    X = data[col]\n",
    "    X = np.atleast_2d(X).T\n",
    "    y = data['SalePrice']\n",
    "    \n",
    "    # remove non-filled rows\n",
    "    isnan_mask = np.any(np.isnan(X), axis=1) | np.isnan(y)\n",
    "    X = X[~isnan_mask]\n",
    "    y = y[~isnan_mask]\n",
    "    \n",
    "    w = compute_weights_multivariate(X, y)\n",
    "    \n",
    "    # note that we must construct the same feature vector to compute predictions\n",
    "    # i.e. standardize and append column of 1s\n",
    "    X = prepare_features(X)\n",
    "    print('RMS error when using a single column \"{col}\" as regressor is {error:.2f}'\n",
    "        .format(\n",
    "            col=col,\n",
    "            error=np.sqrt(mean_squared_error(y, np.dot(X, w)))\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could now easily combine multiple features for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cols in combinations(columns, 2): \n",
    "    X = data[list(cols)]\n",
    "#     X = np.atleast_2d(X).T\n",
    "    y = data['SalePrice']\n",
    "    \n",
    "    # remove non-filled rows\n",
    "    isnan_mask = np.any(np.isnan(X), axis=1) | np.isnan(y)\n",
    "    X = X[~isnan_mask]\n",
    "    y = y[~isnan_mask]\n",
    "    \n",
    "    w = compute_weights_multivariate(X, y)\n",
    "    \n",
    "    # note that we must construct the same feature vector to compute predictions\n",
    "    # i.e. standardize and append column of 1s\n",
    "    X = prepare_features(X)\n",
    "    print('RMS error when using a columns \"{col}\" as regressor is {error:.2f}'\n",
    "        .format(\n",
    "            col=cols,\n",
    "            error=np.sqrt(mean_squared_error(y, np.dot(X, w)))\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariante linear regression using the Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the gradient descent, we must \n",
    "* initialize the weights $\\mathbf{w}$ somehow,\n",
    "* find a way of computing the __gradient__ of our quality measure $L(\\mathbf{y}, \\widehat{\\mathbf{y}})$ w.r.t. $\\mathbf{w}$,\n",
    "* starting from the initialization, iteratively update weights using the gradient descent: \n",
    "$$\n",
    "\\mathbf{w}^{(i+1)} \\leftarrow \\mathbf{w}^{(i)} - \\gamma \\nabla_{\\mathbf{w}} L\n",
    "$$\n",
    "\n",
    "Since we choose $L(\\mathbf{y}, \\widehat{\\mathbf{y}}) \\equiv \\frac 1 \\ell ||\\mathbf{y} - \\mathbf{X} \\mathbf{w} ||^2$, our gradient is $ \\frac 2 \\ell (\\mathbf{y} - \\mathbf{X} \\mathbf{w}) \\mathbf{X} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w):\n",
    "    \"\"\"\n",
    "    Computes the gradient of MSE loss \n",
    "    for multivariate linear regression of X onto y \n",
    "    w.r.t. w, evaluated at the current w.\n",
    "    \"\"\"\n",
    "    return <your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_init, iterations=1, gamma=0.01):\n",
    "    \"\"\"\n",
    "    Performs the required number of iterations of gradient descent.\n",
    "    Parameters:\n",
    "        X [n_objects, n_features]: matrix of featues\n",
    "        y [n_objects]: responce (dependent) variable\n",
    "        w_init: the value of w used as an initializer\n",
    "        iterations: number of steps for gradient descent to compute\n",
    "        gamma: learning rate (gradient multiplier)\n",
    "    \"\"\"\n",
    "    costs, ws = [], []\n",
    "    w = w_init\n",
    "    for i in range(iterations):\n",
    "        # Compute our cost in current point (before the gradient step)\n",
    "        costs.append(mean_squared_error(y, np.dot(X, w)) / len(y))\n",
    "        \n",
    "        # Remember our weights w in current point\n",
    "        ws.append(w)\n",
    "        \n",
    "        # Compute gradient for w\n",
    "        w_grad = <your code here>\n",
    "        \n",
    "        # Update the current weight w using the formula above (see comments)\n",
    "        w = <your code here>\n",
    "        \n",
    "    return costs, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How run the gradient descent to find the optimal weights vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6a7d5687-9216-0cb4-2543-ed7634fedeb7",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "LPJWCYdFwo8L",
    "outputId": "7fce309d-eac5-4adf-9d4d-77875ed9f6f7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = data['GrLivArea']\n",
    "X = np.atleast_2d(X).T\n",
    "X = prepare_features(X)\n",
    "y = data['SalePrice']\n",
    "\n",
    "gamma = 0.01  # Step size\n",
    "iterations = 200  # No. of iterations\n",
    "np.random.seed(123)  # Set the seed\n",
    "w_init = np.random.rand(X.shape[1]) #Pick some random values to start with\n",
    "\n",
    "# Pass the relevant variables to the function and get the new values back...\n",
    "costs, ws = gradient_descent(X, y, w_init, iterations, gamma)\n",
    "w = ws[-1]\n",
    "\n",
    "# Print the results...\n",
    "print(\"Gradient Descent: {:.2f}, {:.2f}\".format(w[0], w[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following 3 code cells display the cost function and its progress as the learning continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "66654649-f2df-fa87-60f7-0498374b9fe3",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "K9YkCu_ewo8O",
    "outputId": "8c3cbf4b-feea-415e-c4b3-418d3b927b3a"
   },
   "outputs": [],
   "source": [
    "#Plot the cost function...\n",
    "plt.title('Cost Function J')\n",
    "plt.xlabel('No. of iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.plot(costs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Animation\n",
    "def animate_to_gif(X, y, costs, out_filename='animation.gif'):\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes()\n",
    "    plt.title('Sale Price vs Living Area')\n",
    "    plt.xlabel('Living Area in square feet (normalised)')\n",
    "    plt.ylabel('Sale Price ($)')\n",
    "    plt.scatter(X[:,1], y, color='red')\n",
    "    line, = ax.plot([], [], lw=2)\n",
    "    annotation = ax.text(-1, 700000, '')\n",
    "    annotation.set_animated(True)\n",
    "    plt.close()\n",
    "\n",
    "    #Generate the animation data,\n",
    "    def init():\n",
    "        line.set_data([], [])\n",
    "        annotation.set_text('')\n",
    "        return line, annotation\n",
    "\n",
    "    # animation function.  This is called sequentially\n",
    "    def animate(i):\n",
    "        x = np.linspace(-5, 20, 1000)\n",
    "        y = ws[i][1]*x + ws[i][0]\n",
    "        line.set_data(x, y)\n",
    "        annotation.set_text('Cost = %.2f e10' % (past_costs[i]/10000000000))\n",
    "        return line, annotation\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                                   frames=iterations, interval=0, blit=True)\n",
    "\n",
    "    anim.save(out_filename, writer='pillow', fps = 30)\n",
    "    \n",
    "\n",
    "def animation_display(filename='animation.gif'):\n",
    "    #Display the animation...\n",
    "    import io\n",
    "    import base64\n",
    "    from IPython.display import HTML\n",
    "\n",
    "    video = io.open(filename, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    HTML(data='''<img src=\"data:image/gif;base64,{0}\" type=\"gif\" />'''.format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1c2c45b4-4e9e-c242-3ede-295ac0bf24df",
    "colab": {},
    "colab_type": "code",
    "id": "cbxB91Kuwo8U"
   },
   "outputs": [],
   "source": [
    "out_filename = 'animation_gd.gif'\n",
    "\n",
    "animate_to_gif(X, y, costs, out_filename)\n",
    "\n",
    "#Display the animation...\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "video = io.open(out_filename, 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''<img src=\"data:image/gif;base64,{0}\" type=\"gif\" />'''.format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uBtVuVNmxJLZ"
   },
   "source": [
    "### Multivariante linear regression using the Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Stochastic Gradient Descent (SGD), we must do the same as for vanilla GD\n",
    "* initialize the weights $\\mathbf{w}$ somehow,\n",
    "* find a way of computing the __gradient__ of our quality measure $L(\\mathbf{y}, \\widehat{\\mathbf{y}})$ w.r.t. $\\mathbf{w}$,\n",
    "* starting from the initialization, iteratively update weights using the gradient descent: \n",
    "$$\n",
    "\\mathbf{w}^{(i+1)} \\leftarrow \\mathbf{w}^{(i)} - \\gamma \\nabla_{\\mathbf{w}} L\n",
    "$$\n",
    "**but** we now compute the gradient with respect to __a single sample__ $(\\mathbf{x}_i, y_i)$ only.\n",
    "\n",
    "Thus, since we choose $L(y_i, \\widehat{y}_i) \\equiv (y_i - \\mathbf{x}_i^T \\mathbf{w} )^2$, \n",
    "our gradient is $ 2 (y_i - \\mathbf{x}_i^T \\mathbf{w}) \\mathbf{x}_i $.\n",
    "\n",
    "In fact, we can still use the already implemented `compute_gradient` function, but now we should only pass inside the subset (one) input vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X, y, w_init, iterations=1, gamma=0.01):\n",
    "    \"\"\"\n",
    "    Performs the required number of iterations of stochastic gradient descent.\n",
    "    Parameters:\n",
    "        X [n_objects, n_features]: matrix of featues\n",
    "        y [n_objects]: responce (dependent) variable\n",
    "        w_init: the value of w used as an initializer\n",
    "        iterations: number of steps for gradient descent to compute\n",
    "        gamma: learning rate (gradient multiplier)\n",
    "    \"\"\"\n",
    "    costs, ws = [], []\n",
    "    w = w_init\n",
    "    for i in range(iterations):\n",
    "        # Compute our cost in current point (before the gradient step)\n",
    "        costs.append(mean_squared_error(y, np.dot(X, w)) / len(y))\n",
    "        \n",
    "        # Remember our weights w in current point\n",
    "        ws.append(w)\n",
    "        \n",
    "        # Select a random sample from the dataset\n",
    "        i = <your code here>\n",
    "        \n",
    "        # Compute gradient for w\n",
    "        w_grad = <your code here>\n",
    "        \n",
    "        # Update the current weight w using the formula above (see comments)\n",
    "        w = <your code here>\n",
    "        \n",
    "    return costs, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['GrLivArea']\n",
    "X = np.atleast_2d(X).T\n",
    "X = prepare_features(X)\n",
    "y = data['SalePrice']\n",
    "\n",
    "gamma = 0.01  # Step size\n",
    "iterations = 200  # No. of iterations\n",
    "np.random.seed(123)  # Set the seed\n",
    "w_init = np.random.rand(X.shape[1]) #Pick some random values to start with\n",
    "\n",
    "# Pass the relevant variables to the function and get the new values back...\n",
    "costs, ws = stochastic_gradient_descent(X, y, w_init, iterations, gamma)\n",
    "w = ws[-1]\n",
    "\n",
    "# Print the results...\n",
    "print(\"Gradient Descent: {:.2f}, {:.2f}\".format(w[0], w[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the cost function is not as smooth anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the cost function...\n",
    "plt.title('Cost Function J')\n",
    "plt.xlabel('No. of iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.plot(costs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note how, along with training, the line estimate is more noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_filename = 'animation_sgd.gif'\n",
    "\n",
    "animate_to_gif(X, y, costs, out_filename)\n",
    "\n",
    "#Display the animation...\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "video = io.open(out_filename, 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''<img src=\"data:image/gif;base64,{0}\" type=\"gif\" />'''.format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Validation split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We commonly set aside a portion of data for testing our model. In typical scenarios, this corresponds to n-fold cross-validation, but here for the sake of simplicity we only show the train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[columns]\n",
    "y = data['SalePrice']\n",
    "\n",
    "# this is common as many columns tend to be empty for some reason\n",
    "isnan_mask = np.any(np.isnan(X), axis=1) | np.isnan(y)\n",
    "X = X[~isnan_mask].values\n",
    "y = y[~isnan_mask].values\n",
    "# X = prepare_features(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = compute_weights_multivariate(X_train, y_train)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse = np.sqrt(mean_squared_error(y_train, np.dot(prepare_features(X_train), w)))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, np.dot(prepare_features(X_test), w)))\n",
    "\n",
    "print('Train error: ', train_rmse)\n",
    "print('Test error: ', test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that for many cases, validation-set MSE will be very different from MSE computed on train test. Regularization could be seen as one of the ways of tuning (additionally penalizing) the model for being _too well_ fitted on training data. Thus, performance on the validation set is more consistent with that on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized multivariate regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regularized least squares are given by the formula:\n",
    "$$\n",
    "\\mathbf{w}^* =\n",
    "  ( \\mathbf{X}^{\\intercal} \\mathbf{X} + \\alpha \\mathbf{I})^{-1}\n",
    "  \\mathbf{X}^{\\intercal} \\mathbf{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weights_multivariate_l2reg(X, y, reg=1e-3):\n",
    "    \"\"\"\n",
    "    Given feature array X [n_samples, 1], target vector y [n_samples],\n",
    "    compute the optimal least squares solution using the formulae above.\n",
    "    Don't forget the bias term!\n",
    "    \"\"\"\n",
    "    return <your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_reg = compute_weights_multivariate_l2reg(X_train, y_train, reg=0)\n",
    "\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, np.dot(prepare_features(X_train), w_reg)))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, np.dot(prepare_features(X_test), w_reg)))\n",
    "\n",
    "print('Train error: ', train_rmse)\n",
    "print('Test error: ', test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_values = []\n",
    "train_rmses, test_rmses = [], []\n",
    "for reg in np.linspace(0.1, 100, 100):\n",
    "    w_reg = compute_weights_multivariate_l2reg(X_train, y_train, reg=reg)\n",
    "    reg_values.append(reg)\n",
    "\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, np.dot(prepare_features(X_train), w_reg)))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, np.dot(prepare_features(X_test), w_reg)))\n",
    "    train_rmses.append(train_rmse)\n",
    "    test_rmses.append(test_rmse)\n",
    "    \n",
    "plt.plot(reg_values, train_rmses, 'r.', label='Train set')\n",
    "plt.plot(reg_values, test_rmses, 'k.', label='Validation set')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, if we choose $\\alpha \\approx 50$, we will obtain lower performance on training set, but a more consistent quality estimate for previously unseen instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One question that's remaining is -- __how to choose $\\alpha$, in practice__?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate regression with a prior on weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the expression for Bayesian regression is the same compared to L2 regression. Yet, it has an important distinction -- the weights prior: $\\mathbf{w} \\sim \\mathcal{N}(0, \\Sigma_w)$. \n",
    "\n",
    "Let $\\Sigma_w = \\sigma^2_w \\mathbf{I}$. If we estimate $\\sigma^2_w$ using the training set, we end up having a more principled way of estimating $\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10)\n",
    "ws = []\n",
    "for train_idx, test_idx in kfold.split(X, y):\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    w = compute_weights_multivariate(X_train, y_train)\n",
    "    ws.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(ws, axis=0), np.std(ws, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.std(ws, axis=0))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "04-linear-regression-from-scratch-gradient-descent.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nteract": {
   "version": "0.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
